# April 4th, 2024 4:37 AM

Right. So I was trying to get the Gemma model to run locally. And, it sort of did when the max length was 10. But anything above that just kind of went on indefinitely. So my guess right now and I'm not 100% sure on this but my guess is that the my computer is just unable to generate such a large response in in such a short amount of time.

And also it is just using the CPU from my understanding, but like this is a very nice computer. It's still a three thousand dollar MacBook Pro. I'm not sure why I just wouldn't be able to render 100 max_tokens, so, I probably don't know exactly why that's working or more. Specifically, I don't know why it's doing what it's doing.

So, my guess, or the next thing I want to try to do here is turn it into a stream. I remember one of my frustrations with using Chat GPT or open AI, or sorry, Bing's Co-pilot was that it was just frustrating to see the text slowly get generated onto this the screen. Originally, I thought it was a design choice, but now I'm, I'm kind of seeing that it's actually, a technical limitation, that it actually takes time to generate each token. So, we're going to
try and put in the Stream.
